{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.14.3"
    },
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damerei/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/LS_DS_Unit_4_Sprint_Challenge_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04CbwdhgwWM1",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2*\n",
        "\n",
        "# Sprint Challenge - Neural Network Foundations\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. [Defining Neural Networks](#Q1)\n",
        "2. [Perceptron on XOR Gates](#Q2)\n",
        "3. [Multilayer Perceptron](#Q3)\n",
        "4. [Keras MMP](#Q4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KLe1e9hwWM2",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. Define the following terms:\n",
        "\n",
        "- **Neuron:** A neuron - perhaps more accurately an artificial neuron - is a conceptual descriptor for a node in the mathematical artifice we call a neural network. \n",
        "- **Input Layer:** The input layer is the set of neurons representing the input data. For example, in the typical case of an image, a natural representation is to treat each pixel as an input corresponding to a neuron.\n",
        "- **Hidden Layer:** The hidden layers are one or more layers of artificial neurons which represent various compressions, encodings, or otherwise transformations of the input layer, generally according to an activation function.\n",
        "- **Output Layer:** The output layer represents as artificial neurons the results of the transformations encoded in the artificial neural network. \n",
        "- **Activation:** The activation function of a node (aka artificial neuron) is what transforms the input into an output. In the simplest feedforward network, you have an input layer which is transformed through a single application of the activation function in a single hidden layer. Typically you want to choose a function that is continuous (because differentiable and therefore restricted to giving appropriately small output variation in response to input variation) and monotonic (so that input-output variation is consistent in parity). Typical examples are sigmoid or arctan functions. \n",
        "- **Backpropagation:** Backpropagation is an algorithm - closely related to traditional numerical approximation by gradient methods such as the Gauss-Newton algorithm - to allow for progressive, recursive-iterative, and self-engineered improvement of a neural network's modeling effectiveness. Essentially one is constantly searching for the gradient of the error function (which in turn consists of the partial derivatives of the error function with respect to the weights), since this represents the rate of greatest change. Intuitively, by moving in that direction, one hypothetically reduces the error function at the greatest rate. Repeated application does not guarantee locating a global minimum, but certainly is helpful, and a number of techniques such as boosting and momentum exist to help prevent being caught in local minima. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syStI8EQwWM3",
        "colab_type": "text"
      },
      "source": [
        "## 2. Perceptron on XOR Gates <a id=\"Q3=2\"></a>\n",
        "\n",
        "Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "|x1\t|x2|x3|\ty|\n",
        "|---|---|---|---|\n",
        "1|\t1|\t1|\t1|\n",
        "1|\t0|\t1|\t0|\n",
        "0|\t1|\t1|\t0|\n",
        "0|\t0|\t1|\t0|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "J_MXGRegwWM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Establish Inputs\n",
        "inputs = np.array([\n",
        "    [1,1,1],\n",
        "    [1,0,1],\n",
        "    [0,1,1],\n",
        "    [0,0,1]\n",
        "])\n",
        "\n",
        "# Establish Target \n",
        "target = [[1], \n",
        "          [0], \n",
        "          [0], \n",
        "          [0]]\n",
        "\n",
        "# Sigmoid functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    sx = sigmoid(x)\n",
        "    return sx* (1-sx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp1FYZQJylp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = np.random.random((3,1)) - 1\n",
        "\n",
        "weighted_sum = np.dot(inputs, weights)\n",
        "\n",
        "activated_output = sigmoid(weighted_sum)\n",
        "\n",
        "error = target - activated_output\n",
        "\n",
        "adjustments = error * sigmoid_derivative(activated_output)\n",
        "\n",
        "weights += np.dot(inputs.T, adjustments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUKmds2Cy78W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "76f79ece-822a-4b9f-e2dd-c455f4786ae5"
      },
      "source": [
        "weights = np.random.random((3,1)) - 1\n",
        "\n",
        "for iteration in range(10000):\n",
        "    \n",
        "    # Weighted Sum of inputs/weights\n",
        "    \n",
        "    weighted_sum = np.dot(inputs, weights)\n",
        "    \n",
        "    # Activate!\n",
        "    activated_output = sigmoid(weighted_sum)\n",
        "    \n",
        "    # Calculate the Error\n",
        "    error = target - activated_output\n",
        "    \n",
        "    # Adjustments\n",
        "    adjustments = error * sigmoid_derivative(activated_output)\n",
        "    \n",
        "    # New weights. \n",
        "    weights += np.dot(inputs.T, adjustments)\n",
        "    \n",
        "print('Weights after Training')\n",
        "print(weights)\n",
        "\n",
        "print('Outputs after training')\n",
        "print(activated_output)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights after Training\n",
            "[[ 11.840066  ]\n",
            " [ 11.840066  ]\n",
            " [-18.04840824]]\n",
            "Outputs after training\n",
            "[[9.96430036e-01]\n",
            " [2.00873011e-03]\n",
            " [2.00873011e-03]\n",
            " [1.45146560e-08]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr-cAPWazJL4",
        "colab_type": "text"
      },
      "source": [
        "These outputs are substantially the target [1, 0, 0, 0], differing in most cases by a few orders of magnitude. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDOMsjmSzDKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS5B0qUvwWM6",
        "colab_type": "text"
      },
      "source": [
        "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
        "\n",
        "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
        "Your network must have one hidden layer.\n",
        "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "Train your model on the Heart Disease dataset from UCI:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "9Q6J4aFkwWM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(object):\n",
        "    def __init__(self,\n",
        "                inputLayerSize=4,\n",
        "                outputLayerSize=1,\n",
        "                hiddenLayerSize=4):\n",
        "        \n",
        "        #Define Hyperparameters\n",
        "        self.inputLayerSize = inputLayerSize\n",
        "        self.outputLayerSize = outputLayerSize\n",
        "        self.hiddenLayerSize = hiddenLayerSize\n",
        "        \n",
        "        #Weights (parameters)\n",
        "        #Input Layer\n",
        "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
        "        # Hidden Layers\n",
        "        self.Wh = np.random.randn(self.hiddenLayerSize,self.hiddenLayerSize)\n",
        "        # Output Layer\n",
        "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Propagate inputs though network\n",
        "        \"\"\"\n",
        "        # Input/1st Hidden Layer\n",
        "        self.z2 = np.dot(X, self.W1)\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        # 2nd Hidden Layer\n",
        "        self.zh = np.dot(self.a2, self.Wh)\n",
        "        self.a3 = self.sigmoid(self.zh)\n",
        "        # Output Layer\n",
        "        self.z3 = np.dot(self.a3, self.W2)\n",
        "        yHat = self.sigmoid(self.z3) \n",
        "        return yHat\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
        "        return 1/(1+np.exp(-z))\n",
        "    \n",
        "    def sigmoidPrime(self,z):\n",
        "        #Gradient of sigmoid\n",
        "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
        "    \n",
        "    def costFunction(self, X, y):\n",
        "        #Compute cost for given X,y, use weights already stored in class.\n",
        "        self.yHat = self.forward(X)\n",
        "        J = 0.5*sum((y-self.yHat)**2)\n",
        "        return J\n",
        "        \n",
        "    def costFunctionPrime(self, X, y):\n",
        "        #Compute derivative with respect to W and W2 for a given X and y:\n",
        "        self.yHat = self.forward(X)\n",
        "        \n",
        "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
        "        dJdW2 = np.dot(self.a2.T, delta3)\n",
        "        \n",
        "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
        "        dJdW1 = np.dot(X.T, delta2)  \n",
        "        \n",
        "        return dJdW1, dJdW2\n",
        "    \n",
        "    #Helper Functions for interacting with other classes:\n",
        "    def getParams(self):\n",
        "        #Get W1 and W2 unrolled into vector:\n",
        "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
        "        return params\n",
        "    \n",
        "    def setParams(self, params):\n",
        "        #Set W1 and W2 using single paramater vector.\n",
        "        W1_start = 0\n",
        "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
        "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
        "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
        "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
        "        \n",
        "    def computeGradients(self, X, y):\n",
        "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
        "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
        "    \n",
        "\n",
        "class trainer(object):\n",
        "    def __init__(self, N):\n",
        "        #Make Local reference to network:\n",
        "        self.N = N\n",
        "        \n",
        "    def callbackF(self, params):\n",
        "        self.N.setParams(params)\n",
        "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
        "        \n",
        "    def costFunctionWrapper(self, params, X, y):\n",
        "        self.N.setParams(params)\n",
        "        cost = self.N.costFunction(X, y)\n",
        "        grad = self.N.computeGradients(X,y)\n",
        "        \n",
        "        return cost, grad\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        #Make an internal variable for the callback function:\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        #Make empty list to store costs:\n",
        "        self.J = []\n",
        "        \n",
        "        params0 = self.N.getParams()\n",
        "\n",
        "        options = {'maxiter': 200, 'disp' : True}\n",
        "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
        "                                 args=(X, y), options=options, callback=self.callbackF)\n",
        "\n",
        "        self.N.setParams(_res.x)\n",
        "        self.optimizationResults = _res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cXTQqwazd5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import optimize\n",
        "class trainer(object):\n",
        "    def __init__(self, N):\n",
        "        #Make Local reference to network:\n",
        "        self.N = N\n",
        "        \n",
        "    def callbackF(self, params):\n",
        "        self.N.setParams(params)\n",
        "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
        "        \n",
        "    def costFunctionWrapper(self, params, X, y):\n",
        "        self.N.setParams(params)\n",
        "        cost = self.N.costFunction(X, y)\n",
        "        grad = self.N.computeGradients(X,y)\n",
        "        \n",
        "        return cost, grad\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        #Make an internal variable for the callback function:\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        #Make empty list to store costs:\n",
        "        self.J = []\n",
        "        \n",
        "        params0 = self.N.getParams()\n",
        "\n",
        "        options = {'maxiter': 200, 'disp' : True}\n",
        "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
        "                                 args=(X, y), options=options, callback=self.callbackF)\n",
        "\n",
        "        self.N.setParams(_res.x)\n",
        "        self.optimizationResults = _res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79vBtifA0cOq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ee0fb4bb-29e2-486c-f43b-6b8c9f9b8cee"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "0   63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
              "1   37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
              "2   41    0   1       130   204    0  ...      0      1.4      2   0     2       1\n",
              "3   56    1   1       120   236    0  ...      0      0.8      2   0     2       1\n",
              "4   57    0   0       120   354    0  ...      1      0.6      2   0     2       1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHIC08AL-KTm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e354467-ada0-46ad-d8d1-f5bdf1cfe5a0"
      },
      "source": [
        "X = df.drop(columns='target').values\n",
        "y = df[['target']].values\n",
        "\n",
        "target.shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(303,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4TvBszj-boQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "83bc0adf-8afa-408b-f82b-1f65356c5007"
      },
      "source": [
        "p2 = MLP(inputLayerSize=X.shape[1])\n",
        "tp = trainer(p2)\n",
        "tp.train(X,y)\n",
        "\n",
        "y_pred = np.around(MLP.forward(p2 , X))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 41.355619\n",
            "         Iterations: 0\n",
            "         Function evaluations: 1\n",
            "         Gradient evaluations: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: overflow encountered in square\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM5Hz5rPOfoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f7bc834-7330-4d11-9006-d7942ab7a960"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_pred, y)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5445544554455446"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfomVnuawWM8",
        "colab_type": "text"
      },
      "source": [
        "## 4. Keras MMP <a id=\"Q4\"></a>\n",
        "\n",
        "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
        "Use the Heart Disease Dataset (binary classification)\n",
        "Use an appropriate loss function for a binary classification task\n",
        "Use an appropriate activation function on the final layer of your network.\n",
        "Train your model using verbose output for ease of grading.\n",
        "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
        "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi6NptEDAPJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "55548a1b-e60d-456f-f256-be77c96b0f64"
      },
      "source": [
        "!pip install category_encoders"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/a1/f7a22f144f33be78afeb06bfa78478e8284a64263a3c09b1ef54e673841e/category_encoders-2.0.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.0)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.16.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.21.2)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.24.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2S3po0vAWMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0fe25a24-7eae-4be6-9ddd-219d7f163db6"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "0   63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
              "1   37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
              "2   41    0   1       130   204    0  ...      0      1.4      2   0     2       1\n",
              "3   56    1   1       120   236    0  ...      0      0.8      2   0     2       1\n",
              "4   57    0   0       120   354    0  ...      1      0.6      2   0     2       1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "lhBB7TBswWM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "import category_encoders as ce"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuK_LgOH_mtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.drop(columns=['target']).values\n",
        "y = df['target'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaJTFTqiAlxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def base_pipe(tts=False, X=X, y=y):\n",
        "    \"\"\"A basic pipeline for transforming the data\"\"\"\n",
        "    \n",
        "    ord_enc = ce.OrdinalEncoder()\n",
        "    scaler  = StandardScaler()\n",
        "\n",
        "\n",
        "    X = ord_enc.fit_transform(X)\n",
        "    X = scaler.fit_transform(X)\n",
        "    \n",
        "    \n",
        "    if tts==True:\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                            test_size=0.3, \n",
        "                                                            random_state=42)\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "    \n",
        "    else:\n",
        "        \n",
        "        return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn26xUmuAuL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6a70f0b-ed3b-4c8d-d000-3e3db5adb5ca"
      },
      "source": [
        "X_train, X_test, y_train, y_test = base_pipe(tts=True)\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((212, 13), (91, 13), (212,), (91,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AavF5u4FAx1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define model function for Keras Classifier Object\n",
        "# create model\n",
        "model = Sequential()\n",
        "# Input and First Hidden Layer\n",
        "model.add(Dense(32, input_dim=X.shape[1], activation='relu'))\n",
        "# Second Hidden Layer\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# Output Layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='rmsprop', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oEirV3FA_y3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "75e7cc61-42d9-4af4-deac-2cefae7892e7"
      },
      "source": [
        "model_history = model.fit(X_train, y_train,\n",
        "                          epochs=100,\n",
        "                          batch_size=64,\n",
        "                          validation_data=(X_test, y_test),\n",
        "                          verbose=0)\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print('Neural Network ACC: ', scores[1])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61/61 [==============================] - 0s 102us/step\n",
            "Neural Network ACC:  0.524590154163173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoFBNWcWMVH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LeakyReLU = keras.layers.LeakyReLU(alpha=0.3)\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "# Input and First Hidden Layer\n",
        "model.add(Dense(64, input_dim=X.shape[1], activation='relu'))\n",
        "# LeakyReLU Advanced function layer. \n",
        "# See https://github.com/keras-team/keras/issues/2272#issuecomment-209001884\n",
        "model.add(LeakyReLU)\n",
        "# Second Hidden Layer\n",
        "model.add(Dense(64, activation='sigmoid'))\n",
        "# Third Hidden Layer\n",
        "model.add(Dense(32, activation='sigmoid'))\n",
        "# Output Layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR66qnz8MmnE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "29115793-bf14-4caa-8c03-03f2ce62542b"
      },
      "source": [
        "model_history = model.fit(X_train, y_train,\n",
        "                          epochs=100,\n",
        "                          batch_size=64,\n",
        "                          validation_data=(X_test, y_test),\n",
        "                          verbose=0)\n",
        "\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print('Neural Network ACC: ', scores[1])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61/61 [==============================] - 0s 50us/step\n",
            "Neural Network ACC:  0.8688524609706441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0NmvOT9Mo7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define model function for Keras Classifier Object\n",
        "def create_model():\n",
        "    \n",
        "    LeakyReLU = keras.layers.LeakyReLU(alpha=0.3)\n",
        "\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    # Input and First Hidden Layer\n",
        "    model.add(Dense(64, input_dim=X.shape[1], activation='relu'))\n",
        "    # LeakyReLU Advanced function layer. \n",
        "    # See https://github.com/keras-team/keras/issues/2272#issuecomment-209001884\n",
        "    model.add(LeakyReLU)\n",
        "    # Second Hidden Layer\n",
        "    model.add(Dense(64, activation='sigmoid'))\n",
        "    # Third Hidden Layer\n",
        "    model.add(Dense(32, activation='sigmoid'))\n",
        "    # Output Layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', \n",
        "                  optimizer='adam', \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krSXL2IbMtZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "c370a870-319f-4111-ef1e-dade7649f91c"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Define the grid search parameters\n",
        "param_grid = {'batch_size': [10, 40, 80, 120],\n",
        "              'epochs': [20, 50, 100 , 200, 400, 600]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid,\n",
        "                    cv=5,\n",
        "                    n_jobs=-1)\n",
        "\n",
        "grid_result = grid.fit(X, y, verbose=0)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.7491749165081741 using {'batch_size': 10, 'epochs': 100}\n",
            "Means: 0.44554455986510805, Stdev: 0.2078515207231098 with: {'batch_size': 10, 'epochs': 20}\n",
            "Means: 0.6831683205692681, Stdev: 0.0672472366711982 with: {'batch_size': 10, 'epochs': 50}\n",
            "Means: 0.7491749165081741, Stdev: 0.17598905598592587 with: {'batch_size': 10, 'epochs': 100}\n",
            "Means: 0.6963696420782863, Stdev: 0.07223526382205561 with: {'batch_size': 10, 'epochs': 200}\n",
            "Means: 0.6831683261756456, Stdev: 0.060439423997315424 with: {'batch_size': 10, 'epochs': 400}\n",
            "Means: 0.6666666725681166, Stdev: 0.05605020195118876 with: {'batch_size': 10, 'epochs': 600}\n",
            "Means: 0.3894389481237619, Stdev: 0.24925199860587943 with: {'batch_size': 40, 'epochs': 20}\n",
            "Means: 0.5379538021662055, Stdev: 0.17844677452732335 with: {'batch_size': 40, 'epochs': 50}\n",
            "Means: 0.6468646927635269, Stdev: 0.10456819283874651 with: {'batch_size': 40, 'epochs': 100}\n",
            "Means: 0.554455440528322, Stdev: 0.09804963434997392 with: {'batch_size': 40, 'epochs': 200}\n",
            "Means: 0.6633663478464183, Stdev: 0.05706992745912721 with: {'batch_size': 40, 'epochs': 400}\n",
            "Means: 0.6171617144011822, Stdev: 0.08728099218145066 with: {'batch_size': 40, 'epochs': 600}\n",
            "Means: 0.25742573568923244, Stdev: 0.22421308177902266 with: {'batch_size': 80, 'epochs': 20}\n",
            "Means: 0.3960395985015548, Stdev: 0.21787843149774586 with: {'batch_size': 80, 'epochs': 50}\n",
            "Means: 0.49504950908151, Stdev: 0.2355966845757821 with: {'batch_size': 80, 'epochs': 100}\n",
            "Means: 0.5940593985637815, Stdev: 0.1652066776493152 with: {'batch_size': 80, 'epochs': 200}\n",
            "Means: 0.7425742538848726, Stdev: 0.1341421005338111 with: {'batch_size': 80, 'epochs': 400}\n",
            "Means: 0.6831683105368032, Stdev: 0.08146069446630505 with: {'batch_size': 80, 'epochs': 600}\n",
            "Means: 0.32673266775930676, Stdev: 0.3057512314220992 with: {'batch_size': 120, 'epochs': 20}\n",
            "Means: 0.46534653288303035, Stdev: 0.26661334416512333 with: {'batch_size': 120, 'epochs': 50}\n",
            "Means: 0.5049504893447699, Stdev: 0.1450482494194853 with: {'batch_size': 120, 'epochs': 100}\n",
            "Means: 0.5445544524948196, Stdev: 0.1220240486867178 with: {'batch_size': 120, 'epochs': 200}\n",
            "Means: 0.5544554457412695, Stdev: 0.11615148515831338 with: {'batch_size': 120, 'epochs': 400}\n",
            "Means: 0.6435643599765135, Stdev: 0.11079418323226065 with: {'batch_size': 120, 'epochs': 600}\n",
            "CPU times: user 6.46 s, sys: 346 ms, total: 6.8 s\n",
            "Wall time: 6min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRbfb7fBMywL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1af007ae-2e4d-43a5-e508-2a85f9e1121e"
      },
      "source": [
        "best_batch = grid_result.best_params_['batch_size']\n",
        "best_epoch = grid_result.best_params_['epochs']\n",
        "\n",
        "optimal_model = create_model()\n",
        "\n",
        "opt = optimal_model.fit(X_train, y_train,\n",
        "                        epochs=best_epoch,\n",
        "                        batch_size=best_batch,\n",
        "                        validation_data=(X_test, y_test),\n",
        "                        verbose=1)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 242 samples, validate on 61 samples\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9165 - acc: 0.4504 - val_loss: 0.7051 - val_acc: 0.4754\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 188us/step - loss: 0.6864 - acc: 0.5372 - val_loss: 0.6774 - val_acc: 0.5246\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 159us/step - loss: 0.6763 - acc: 0.5909 - val_loss: 0.6611 - val_acc: 0.6066\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 170us/step - loss: 0.6737 - acc: 0.5909 - val_loss: 0.6644 - val_acc: 0.5738\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 183us/step - loss: 0.6665 - acc: 0.6116 - val_loss: 0.6446 - val_acc: 0.8033\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 170us/step - loss: 0.6654 - acc: 0.6033 - val_loss: 0.6422 - val_acc: 0.7049\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 178us/step - loss: 0.6649 - acc: 0.5744 - val_loss: 0.6483 - val_acc: 0.5246\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 171us/step - loss: 0.6590 - acc: 0.5744 - val_loss: 0.6265 - val_acc: 0.8033\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 172us/step - loss: 0.6570 - acc: 0.6116 - val_loss: 0.6067 - val_acc: 0.8033\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 186us/step - loss: 0.6450 - acc: 0.6860 - val_loss: 0.6103 - val_acc: 0.7869\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 179us/step - loss: 0.6440 - acc: 0.6612 - val_loss: 0.6145 - val_acc: 0.7869\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 164us/step - loss: 0.6371 - acc: 0.6653 - val_loss: 0.5910 - val_acc: 0.7869\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 189us/step - loss: 0.6417 - acc: 0.6777 - val_loss: 0.6014 - val_acc: 0.7869\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 177us/step - loss: 0.6333 - acc: 0.6570 - val_loss: 0.6025 - val_acc: 0.6557\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 176us/step - loss: 0.6259 - acc: 0.6694 - val_loss: 0.5577 - val_acc: 0.7869\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 182us/step - loss: 0.6408 - acc: 0.6446 - val_loss: 0.5583 - val_acc: 0.8197\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 175us/step - loss: 0.6257 - acc: 0.6818 - val_loss: 0.5564 - val_acc: 0.8033\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 180us/step - loss: 0.6133 - acc: 0.6901 - val_loss: 0.5487 - val_acc: 0.8197\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 176us/step - loss: 0.6326 - acc: 0.6694 - val_loss: 0.5634 - val_acc: 0.7705\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 179us/step - loss: 0.6438 - acc: 0.6364 - val_loss: 0.6005 - val_acc: 0.6885\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 187us/step - loss: 0.6233 - acc: 0.6777 - val_loss: 0.5691 - val_acc: 0.8197\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 177us/step - loss: 0.6084 - acc: 0.6694 - val_loss: 0.5449 - val_acc: 0.8197\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 171us/step - loss: 0.5982 - acc: 0.6942 - val_loss: 0.5641 - val_acc: 0.7377\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 173us/step - loss: 0.6188 - acc: 0.6529 - val_loss: 0.5257 - val_acc: 0.8361\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 173us/step - loss: 0.6095 - acc: 0.6860 - val_loss: 0.5486 - val_acc: 0.7541\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 182us/step - loss: 0.6062 - acc: 0.6942 - val_loss: 0.5338 - val_acc: 0.8197\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 168us/step - loss: 0.5913 - acc: 0.7025 - val_loss: 0.5061 - val_acc: 0.7705\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 164us/step - loss: 0.5950 - acc: 0.6942 - val_loss: 0.4951 - val_acc: 0.8361\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 195us/step - loss: 0.5885 - acc: 0.6364 - val_loss: 0.5209 - val_acc: 0.8197\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 156us/step - loss: 0.5914 - acc: 0.6983 - val_loss: 0.5445 - val_acc: 0.6885\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 170us/step - loss: 0.5874 - acc: 0.7273 - val_loss: 0.5164 - val_acc: 0.7869\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 166us/step - loss: 0.5842 - acc: 0.6942 - val_loss: 0.5195 - val_acc: 0.7541\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 167us/step - loss: 0.5864 - acc: 0.6818 - val_loss: 0.4720 - val_acc: 0.8525\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 170us/step - loss: 0.6133 - acc: 0.6405 - val_loss: 0.5078 - val_acc: 0.7869\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 170us/step - loss: 0.5855 - acc: 0.7066 - val_loss: 0.4944 - val_acc: 0.8197\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 164us/step - loss: 0.5905 - acc: 0.6983 - val_loss: 0.4935 - val_acc: 0.8361\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 188us/step - loss: 0.5809 - acc: 0.7025 - val_loss: 0.4801 - val_acc: 0.8525\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 169us/step - loss: 0.5719 - acc: 0.6983 - val_loss: 0.4653 - val_acc: 0.8361\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 164us/step - loss: 0.5740 - acc: 0.7149 - val_loss: 0.5111 - val_acc: 0.7869\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 186us/step - loss: 0.5522 - acc: 0.7314 - val_loss: 0.4653 - val_acc: 0.8525\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 183us/step - loss: 0.5778 - acc: 0.6942 - val_loss: 0.4845 - val_acc: 0.8361\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 162us/step - loss: 0.5753 - acc: 0.6653 - val_loss: 0.4768 - val_acc: 0.8361\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 182us/step - loss: 0.5567 - acc: 0.7273 - val_loss: 0.4796 - val_acc: 0.8361\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 162us/step - loss: 0.5480 - acc: 0.7231 - val_loss: 0.4491 - val_acc: 0.8852\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 178us/step - loss: 0.5639 - acc: 0.7149 - val_loss: 0.4831 - val_acc: 0.8525\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 171us/step - loss: 0.5508 - acc: 0.7066 - val_loss: 0.4833 - val_acc: 0.7705\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 168us/step - loss: 0.5530 - acc: 0.6983 - val_loss: 0.4705 - val_acc: 0.8689\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 169us/step - loss: 0.5494 - acc: 0.7231 - val_loss: 0.5233 - val_acc: 0.7541\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 178us/step - loss: 0.5784 - acc: 0.6942 - val_loss: 0.4557 - val_acc: 0.8197\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 170us/step - loss: 0.5305 - acc: 0.7562 - val_loss: 0.4303 - val_acc: 0.8361\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 180us/step - loss: 0.5673 - acc: 0.6777 - val_loss: 0.4710 - val_acc: 0.8033\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 177us/step - loss: 0.5329 - acc: 0.7190 - val_loss: 0.4252 - val_acc: 0.8689\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 184us/step - loss: 0.5257 - acc: 0.7314 - val_loss: 0.4372 - val_acc: 0.8525\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 169us/step - loss: 0.5206 - acc: 0.7479 - val_loss: 0.4341 - val_acc: 0.8689\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 179us/step - loss: 0.5039 - acc: 0.7479 - val_loss: 0.4233 - val_acc: 0.8689\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 166us/step - loss: 0.5205 - acc: 0.7603 - val_loss: 0.4162 - val_acc: 0.8689\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 172us/step - loss: 0.5291 - acc: 0.7273 - val_loss: 0.4558 - val_acc: 0.7869\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 181us/step - loss: 0.5035 - acc: 0.7603 - val_loss: 0.3950 - val_acc: 0.8852\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 163us/step - loss: 0.5065 - acc: 0.7727 - val_loss: 0.4067 - val_acc: 0.8689\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 191us/step - loss: 0.5234 - acc: 0.7645 - val_loss: 0.4213 - val_acc: 0.8361\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 168us/step - loss: 0.5065 - acc: 0.7562 - val_loss: 0.3865 - val_acc: 0.8689\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 167us/step - loss: 0.5119 - acc: 0.7149 - val_loss: 0.4133 - val_acc: 0.8197\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 166us/step - loss: 0.5378 - acc: 0.7355 - val_loss: 0.4858 - val_acc: 0.7869\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 184us/step - loss: 0.5040 - acc: 0.7479 - val_loss: 0.4128 - val_acc: 0.8689\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 164us/step - loss: 0.4885 - acc: 0.7686 - val_loss: 0.4034 - val_acc: 0.8525\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 165us/step - loss: 0.4741 - acc: 0.7893 - val_loss: 0.3922 - val_acc: 0.8525\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 184us/step - loss: 0.4779 - acc: 0.7603 - val_loss: 0.3870 - val_acc: 0.8361\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 172us/step - loss: 0.4919 - acc: 0.7562 - val_loss: 0.4372 - val_acc: 0.7869\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 168us/step - loss: 0.4921 - acc: 0.7727 - val_loss: 0.3908 - val_acc: 0.8361\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 185us/step - loss: 0.4699 - acc: 0.7893 - val_loss: 0.4157 - val_acc: 0.8033\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 179us/step - loss: 0.4726 - acc: 0.7893 - val_loss: 0.3777 - val_acc: 0.8525\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 184us/step - loss: 0.5074 - acc: 0.7603 - val_loss: 0.3822 - val_acc: 0.8361\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 182us/step - loss: 0.4824 - acc: 0.7769 - val_loss: 0.3642 - val_acc: 0.8689\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 167us/step - loss: 0.4901 - acc: 0.7479 - val_loss: 0.3800 - val_acc: 0.8525\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 166us/step - loss: 0.4493 - acc: 0.8140 - val_loss: 0.3691 - val_acc: 0.8525\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 154us/step - loss: 0.4544 - acc: 0.7975 - val_loss: 0.3871 - val_acc: 0.8525\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 172us/step - loss: 0.4483 - acc: 0.7893 - val_loss: 0.3881 - val_acc: 0.8033\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 177us/step - loss: 0.4406 - acc: 0.8140 - val_loss: 0.3558 - val_acc: 0.8525\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 169us/step - loss: 0.4547 - acc: 0.7810 - val_loss: 0.3624 - val_acc: 0.8689\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 183us/step - loss: 0.4411 - acc: 0.7975 - val_loss: 0.3801 - val_acc: 0.8033\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 166us/step - loss: 0.4298 - acc: 0.8264 - val_loss: 0.5447 - val_acc: 0.7541\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 171us/step - loss: 0.5001 - acc: 0.7479 - val_loss: 0.3888 - val_acc: 0.8197\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 169us/step - loss: 0.4508 - acc: 0.7934 - val_loss: 0.3694 - val_acc: 0.8525\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 163us/step - loss: 0.4206 - acc: 0.8058 - val_loss: 0.4899 - val_acc: 0.7869\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 187us/step - loss: 0.4336 - acc: 0.7851 - val_loss: 0.3795 - val_acc: 0.8689\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 188us/step - loss: 0.4290 - acc: 0.8264 - val_loss: 0.3611 - val_acc: 0.8525\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 195us/step - loss: 0.4376 - acc: 0.8182 - val_loss: 0.4175 - val_acc: 0.8197\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 201us/step - loss: 0.4217 - acc: 0.8182 - val_loss: 0.3514 - val_acc: 0.8361\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 176us/step - loss: 0.4354 - acc: 0.7934 - val_loss: 0.3607 - val_acc: 0.8689\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 174us/step - loss: 0.5133 - acc: 0.7438 - val_loss: 0.3660 - val_acc: 0.8689\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 180us/step - loss: 0.4497 - acc: 0.8099 - val_loss: 0.3707 - val_acc: 0.8689\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 168us/step - loss: 0.4249 - acc: 0.8182 - val_loss: 0.4346 - val_acc: 0.7705\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 184us/step - loss: 0.4309 - acc: 0.8182 - val_loss: 0.3709 - val_acc: 0.8525\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 165us/step - loss: 0.4220 - acc: 0.8140 - val_loss: 0.4345 - val_acc: 0.8033\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 167us/step - loss: 0.4129 - acc: 0.8223 - val_loss: 0.3511 - val_acc: 0.8525\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 0s 172us/step - loss: 0.4175 - acc: 0.7851 - val_loss: 0.3458 - val_acc: 0.8689\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 171us/step - loss: 0.4041 - acc: 0.8347 - val_loss: 0.3580 - val_acc: 0.8361\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 176us/step - loss: 0.4574 - acc: 0.7438 - val_loss: 0.3689 - val_acc: 0.8361\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 0s 176us/step - loss: 0.4312 - acc: 0.8140 - val_loss: 0.3669 - val_acc: 0.8689\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 0s 171us/step - loss: 0.4503 - acc: 0.7727 - val_loss: 0.3409 - val_acc: 0.8689\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1fsHf2pNsgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "997d5aac-3ccb-4a32-f29d-70e107eb2013"
      },
      "source": [
        "scores = optimal_model.evaluate(X_test, y_test)\n",
        "print('Neural Network ACC: ', scores[1])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61/61 [==============================] - 0s 86us/step\n",
            "Neural Network ACC:  0.8688524609706441\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}